<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="[toc] 机器学习的类别监督学习（我们没有告诉机器正确的答案是什么，机器最终得到的只有一个分数，就是它做的好还是不好） ​    我们知道所有的数据的输入和输出都是什么 ​    回归：预测的输出是连续的 ​    分类：预测的输出是离散的，有二分类也有多分类 半监督学习（Semi-supervised Learning） ​    手头上有少量的labeled data，它们标注了图片上哪只是">
<meta property="og:type" content="article">
<meta property="og:title" content="pbbbb">
<meta property="og:url" content="http://example.com/2022/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="pbbbb">
<meta property="og:description" content="[toc] 机器学习的类别监督学习（我们没有告诉机器正确的答案是什么，机器最终得到的只有一个分数，就是它做的好还是不好） ​    我们知道所有的数据的输入和输出都是什么 ​    回归：预测的输出是连续的 ​    分类：预测的输出是离散的，有二分类也有多分类 半监督学习（Semi-supervised Learning） ​    手头上有少量的labeled data，它们标注了图片上哪只是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220313143811645.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315102740895.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315104006747.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315105147361.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220313193702338.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220313194950419.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220313195154353.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220313195423850.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220314000235753.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220319092853459.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220314154934152.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220320020617193.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220320021229308.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220314161246327.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220320021714758.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220320022346049.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220317222235662.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220317222317686.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315110827382.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315112358918.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315112841221.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315114156470.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315114913427.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315115518885.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315152002018.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315152501983.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315153026724.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315153611969.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315154829196.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315155337996.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315161303356.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315162600386.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315164646761.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315164708912.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315221052288.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315224224756.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315223121002.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315223258073.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315224251221.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315224542878.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315230018387.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315230246740.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220315230523127.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316210748012.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316211021497.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316211603232.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316211658093.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316212951325.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316212911022.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220316213701308.png">
<meta property="og:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220322034538581.png">
<meta property="article:published_time" content="2022-03-26T06:31:49.340Z">
<meta property="article:modified_time" content="2022-03-21T19:51:39.714Z">
<meta property="article:author" content="wpb">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/83549/AppData/Roaming/Typora/typora-user-images/image-20220313143811645.png">

<link rel="canonical" href="http://example.com/2022/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title> | pbbbb</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">pbbbb</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wpb">
      <meta itemprop="description" content="除了英俊其他没什么好说的">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="pbbbb">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-26 14:31:49" itemprop="dateCreated datePublished" datetime="2022-03-26T14:31:49+08:00">2022-03-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-22 03:51:39" itemprop="dateModified" datetime="2022-03-22T03:51:39+08:00">2022-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[toc]</p>
<h1 id="机器学习的类别"><a href="#机器学习的类别" class="headerlink" title="机器学习的类别"></a>机器学习的类别</h1><p><strong>监督学习</strong>（我们没有告诉机器正确的答案是什么，机器最终得到的只有一个分数，就是它做的好还是不好）</p>
<p>​    我们知道所有的数据的输入和输出都是什么</p>
<p>​    <strong>回归</strong>：预测的输出是连续的</p>
<p>​    <strong>分类</strong>：预测的输出是离散的，有二分类也有多分类</p>
<p><strong>半监督学习（Semi-supervised Learning）</strong></p>
<p>​    手头上有少量的labeled data，它们标注了图片上哪只是猫哪只是狗；同时又有大量的unlabeled data，它们仅仅只有猫和狗的图片</p>
<p><strong>迁移学习（Transfer Learning）</strong></p>
<p>​    只有少量的有labeled的data；但是我们现在有大量的不相干的data(不是猫和狗的图片，而是一些其他不相干的图片)，在这些大量的data里面，它可能有label也可能没有label</p>
<p>​    Transfer Learning要解决的问题是，这一堆不相干的data可以对结果带来什么样的帮助</p>
<p><strong>无监督学习(Unsupervised Learning)</strong></p>
<p>​    在完全没有任何label的情况下，机器到底能学到什么样的知识</p>
<p><strong>结构化学习(Structured Learning)</strong></p>
<p>​    让机器学会自己去创造一些东西，比如让他们自动的画画</p>
<p><strong>强化学习(Reinforcement Learning)</strong></p>
<p>​    我们没有告诉机器正确的答案是什么，机器最终得到的只有一个分数，就是它做的好还是不好。</p>
<h1 id="机器学习的步骤"><a href="#机器学习的步骤" class="headerlink" title="机器学习的步骤"></a>机器学习的步骤</h1><ol>
<li>自己定义一个带有未知参数的函数（通常叫做模型），这个函数长什么样子是自己定义的，我们的目的就是为了让机器自己学习出这么一笔未知的参数。通常你对这个模型的修改，往往来自于你对于这个模型的理解</li>
<li>定义loss函数，来测量我们的函数输出结果的好坏，也就是来判断当前参数的好坏。</li>
<li>根据loss，梯度下降更新参数，一开始我们的参数都是随机初始化的，随便找的一个点，在算出loss之后，我们对每一个参数都进行求梯度的操作，然后我们沿着梯度的方向走一步，这一步走多大取决去当前的梯度还有就是学习率。</li>
</ol>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220313143811645.png" alt="image-20220313143811645"></p>
<p>然后依次重复这三个步骤，那什么时候停下来呢？</p>
<ol>
<li>你自己定义它需要训练多少epoch</li>
<li>当走到一个梯度等于0的点</li>
</ol>
<h1 id="局部极小点和全局最小点"><a href="#局部极小点和全局最小点" class="headerlink" title="局部极小点和全局最小点"></a>局部极小点和全局最小点</h1><p>局部极小点其实是一个假问题，很少会出现这种情况</p>
<p>导致梯度为零的点不是只有局部极小点，还有可能是鞍点，为什么我们要区分出这两种情况</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315102740895.png" alt="image-20220315102740895"></p>
<p>局部极小点可能无路可走，但是鞍点的话还是有路可走的</p>
<p>那么如何判断局部极小点和鞍点呢？其实就是看loss函数的海森矩阵是否是正定，负定还是有正有负</p>
<blockquote>
<p>​    例子，假设一个模型是 y&#x3D;w1 * w2 * x, 训练集数据是（1， 1）</p>
<p>​    那么判断w1 &#x3D;0， w2&#x3D;0的时候，图像是否在鞍点的位置</p>
<p>步骤就是先写出loss函数，然后计算海森矩阵，带入w1，w2判断即可</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315104006747.png" alt="image-20220315104006747"></p>
<p>特征值有正有负，因此在w1&#x3D;0，w2&#x3D;0的时候是鞍点</p>
<p>同时我们选择特征值为-2时的特征向量作为更新的方向，那么我们就可以继续降低loss</p>
</blockquote>
<p>但是我们通常情况是不会用这种方法的，因为特征数量一般都很多，算海森会非常的麻烦，还会有别的方法帮助我们逃离鞍点。</p>
<p>saddle point跟local minima,谁比较常见？</p>
<p>有一个说法就是，在二维空间中的局部极小点，在三维的情况下有可能是鞍点，那么在三维空间下呢？从三维的空间来看,</p>
<p>是没有路可以走的东西,在高维的空间中是有路可以走的,error surface会不会也一样呢</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315105147361.png" alt="image-20220315105147361"></p>
<p>所以从经验上看起来,其实local minima并没有那么常见,多数的时候,你觉得你train到一个地方,你gradient真的很小,然后所</p>
<p>以你的参数不再update了,往往是因为你卡在了一个saddle point。</p>
<h1 id="让模型更有弹性"><a href="#让模型更有弹性" class="headerlink" title="让模型更有弹性"></a>让模型更有弹性</h1><p>上述的情况是对于简单的模型来说比如，y&#x3D;wx+b这种的，但是实际情况肯定不是这样的，真是的曲线肯定是相当复杂的，</p>
<p>只是模拟出简单的曲线肯定是不行的，那么对于真实情况如何处理？</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220313193702338.png" alt="image-20220313193702338"></p>
<p>假设真实的曲线是红色的曲线，那么他的近似可以看成是一个常数加上一堆sigmoid函数<br>$$<br>y&#x3D;c\frac{1}{1 + e^{-(b+\omega x_1)}}<br>$$<br><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220313194950419.png" alt="image-20220313194950419"></p>
<p>那么有了不同的w和不同的b之后，我们就可以逼近不同的曲线，所以在之前简单的版本里面，可以进化一下</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220313195154353.png" alt="image-20220313195154353"></p>
<p>在上面的式子里，i表示的是有多少个sigmoid函数，j表示的是特征的数量，上面的算式如果用图的方式的话</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220313195423850.png" alt="image-20220313195423850"></p>
<p>这就和神经网络非常相似了。那么更新参数的方法其实还是和以前一样，也是梯度下降</p>
<p>那么我们可以再次的改进，在这个基础上，把结果重复这样的步骤，让结构进行变深一点。就变成了深度学习</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220314000235753.png" alt="image-20220314000235753"></p>
<p>在更新的时候我们往往选择批量梯度下降，每次更新一次参数叫做一次 Update,把所有的 Batch 都看过一遍,叫做一个 Epoch。</p>
<p>那么我们其实也不一定非要使用sigmoid，也可以用relu</p>
<h1 id="网络结构的深度的意义"><a href="#网络结构的深度的意义" class="headerlink" title="网络结构的深度的意义"></a>网络结构的深度的意义</h1><p>我们一开始说,我们想要用 ReLU 或者是 Sigmoid,去逼近一个复杂的 Function,实际上只要够多的 ReLU 够多的 Sigmoid,就</p>
<p>可以逼近任何的 连续的 Function ,我们只要有够多的 Sigmoid,就可以知道够复杂的线段,就可以逼近任何的 Continuous 的 </p>
<p>Function,所以我们只要一排 ReLU 一排 Sigmoid,够多就足够了,<strong>那深的意义到底何在呢</strong></p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220319092853459.png" alt="image-20220319092853459"></p>
<p>这个时候你会发现，在参数数量接近的情况下，只有1层的network，它的error rate是远大于好几层的network的</p>
<p>那么比较胖瘦的network是直接把整个任务都写到了一起，相当于直接检测整个任务，而比较瘦高的network相当于把一个大的任务分成了一个个的小任务。比如现在有四个分类，长头发女生、长头发男生、短头发女生、短头发男生。</p>
<p>但此时，长头发男生的数据是比较少的，短头发女生数据也是比较少的，如果我们直接让network分别出四个类别，这时候效果肯定是比较差的</p>
<p>但考虑到长头发男生和长头发女生的共同特征：长头发，那么如果我们先做的是分别出长头发还是短头发，再分辨出男生还是女生，这样的话我们就可以利用长头发女生的数据来让network学习到如何辨别长头发，可以充分的利用数据</p>
<h1 id="训练结果不好怎么办"><a href="#训练结果不好怎么办" class="headerlink" title="训练结果不好怎么办"></a>训练结果不好怎么办</h1><ol>
<li><p>首先检测在训练集上的结果</p>
<blockquote>
<p>​    如果训练集上的结果不好</p>
<ol>
<li><p>本身的模型太过简单了，比如你要预测的是模型函数可能是y&#x3D;x^2+x^5,但是你一开始设置的模型是y&#x3D;kx+b，那么无论如何，你找到的模型都不会去拟合正确的答案。</p>
<p>在深度学习里面，通常是模型的结构，比如层数或者特征数等。</p>
</li>
<li><p>也有可能是在更新参数的时候，更新的方法optimization不对，比如让函数走到了局部极小点，</p>
</li>
</ol>
<p>如何分别这两种，可以选择不同的模型来判断</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220314154934152.png" alt="image-20220314154934152"></p>
<p>在上面的情况中，56层的模型上的loss，比20层的模型的loss还要高，那么这个就是优化的问题</p>
<ol start="3">
<li>梯度消失</li>
</ol>
<p>如果你使用了sigmoid激活函数，当你把network叠得很深的时候，在靠近input的地方，这些参数的gradient(即对最后loss function的微分)是比较小的；而在比较靠近output的地方，它对loss的微分值会是比较大的</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220320020617193.png" alt="image-20220320020617193"></p>
<p>解决方法可以尝试更换激活函数，比如使用relu函数，或者其他</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220320021229308.png" alt="image-20220320021229308"></p>
</blockquote>
</li>
<li><p>如果训练集上的结果已经好了，那么看测试集上的结果</p>
<blockquote>
<p>​    如果测试集上的结果好，那么说明，这个模型的效果还不错，就完事了</p>
<p>​    那如果说测试集上的结果不好，这时候的情况是训练集上的结果还可以，但是测试集上的结果不好，这个才有可能是overfitting</p>
<p>​    overfitting的解决方法</p>
<ol>
<li><p>增加训练集，或者说对现有的数据进行增强，比如对于图片数据来说，让他们翻转，随机增加亮度，改变颜色等等，要根据你对资料的特性,对你现在要处理的问题的理解,来选择合适的,data augmentation的方式</p>
</li>
<li><p>让模型不要有太大的弹性，给他一些限制</p>
<ol>
<li><p>给它比较少的参数，比如原本的一千个神经元，现在变成一百个，或者共用参数</p>
</li>
<li><p>减少输入的特征数</p>
</li>
<li><p>early stoping</p>
</li>
</ol>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220314161246327.png" alt="image-20220314161246327"></p>
<ol start="4">
<li>drop out</li>
</ol>
<p>在training的时候，每次update参数之前，我们对每一个neuron做sampling(抽样) ，每个neuron都有p%的几率会被丢掉，如果某个neuron被丢掉的话，跟它相连的weight也都要被丢掉</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220320021714758.png" alt="image-20220320021714758"></p>
<p>做完sampling以后，network structure就会变得比较细长了，然后你再去train这个细长的network</p>
<p>当你在training的时候使用dropout，得到的performance其实是会变差的,<strong>但是在testing set上的结果是变好的</strong></p>
<p>testing的时候不做dropout，所有的neuron都要被用到</p>
<p>假设在training的时候，dropout rate是p%，从training data中被learn出来的所有weight都要乘上(1-p%)才能被当做testing的weight使用</p>
<p>解释：</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220320022346049.png" alt="image-20220320022346049"></p>
<p>假设现在的dropout rate是50%，那在training的时候，你总是期望每次update之前会丢掉一半的neuron，就像上图左侧所示，在这种情况下你learn好了一组weight参数，然后拿去testing</p>
<p>但是在testing的时候是没有dropout的，所以如果testing使用的是和training同一组weight，那左侧得到的output z和右侧得到的output z‘，它们的值其实是会相差两倍的</p>
<ol start="5">
<li>正则化</li>
</ol>
<p>我们期待参数$ W_i$越小甚至接近于0的function, 因为这样的话我们可以让我们的曲线变得更加的平滑，一些噪声对这个平滑的的影响就会比较小</p>
<p>当过拟合的时候，拟合函数的系数往往非常大，而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</p>
<p>我们不可能一开始就否定高次项而直接只采用低次线性表达式的model，因为有时候真实数据的确是符合高次项非线性曲线的分布的；而如果一开始直接采用高次非线性表达式的model，就很有可能造成overfitting，在曲线偏折的地方与真实数据的误差非常大。</p>
<p>L1正则化：<img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220317222235662.png" alt="image-20220317222235662"></p>
<p>L2正则化：<img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220317222317686.png" alt="image-20220317222317686"></p>
<p>λ值越大代表后面的那一项影响力越大，我们找到的function就越平滑<strong>这里的λ需要我们手动去调整以取得最好的值</strong></p>
<p>当我们的λ越大的时候，在training data上得到的error其实是越大的，但是这件事情是非常合理的，因为当λ越大的时候，我们就越倾向于考虑w的值而越少考虑error的大小；但是有趣的是，虽然在training data上得到的error越大，但是在testing data上得到的error可能会是比较小的</p>
<p>相对来说，L2要更稳定一些，L1的结果则不那么稳定</p>
</li>
</ol>
<p>除了overfitting之外，造成这种情况的也有可能是mismath，也就是训练集和测试集的分布是不一样的。比如说训练集都是一些颜色偏黄的图片数据，但是测试集上都是一些颜色偏蓝的数据</p>
</blockquote>
</li>
</ol>
<h1 id="训练集，验证集，测试集"><a href="#训练集，验证集，测试集" class="headerlink" title="训练集，验证集，测试集"></a>训练集，验证集，测试集</h1><h1 id="small-batch-和-large-batch"><a href="#small-batch-和-large-batch" class="headerlink" title="small batch 和 large batch"></a>small batch 和 large batch</h1><p>当batch_size大的时候，那么蓄力的时间就比较长，威力比较大，更新的方向比较准确</p>
<p>当batch_size小的时候，那么蓄力的时间就比较短，威力比较小，更新的方向比较不准，update的方向就会曲曲折折的</p>
<p>那么如果我们考虑GPU并行计算的能力的话，下面有一个真正的实验</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315110827382.png" alt="image-20220315110827382"></p>
<p>那么这样看来的话，因为有了GPU的并行计算能力，似乎大的batch_size还是比较好的，只要不是特别的大，那么剩下的就只有好处了。</p>
<p>那么真正的实验结果又是如何的呢？</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315112358918.png" alt="image-20220315112358918"></p>
<p>让人意外的发现，在两个不同的数据集上，都是图像识别的任务，然后，小的batch_size竟然有好的，神奇的事情是,大的 </p>
<p>Batch Size,往往在 Training 的时候,会给你带来比较差的结果。**这个是 Optimization 的问题,代表当你用大的 Batch **</p>
<p><strong>Size 的时候,你的 Optimization 可能会有问题</strong>,小的 Batch Size,Optimization 的结果反而是比较好的,为什么这样子呢</p>
<p>一个可能的解释：假设我们现在用的是full batch，那么当我们走到了一个局部极小点，或者鞍点，就直接停了下来了，如</p>
<p>果这个时候不去看海森矩阵，那么久没办法判断了。</p>
<p>但是假如是 Small Batch 的话,因为我们每次是挑一个 Batch 出来,算它的 Loss,,等于你每一次 Update 你的参数的时候,你用</p>
<p>的 Loss Function 都是有差异的,你选到第一个 Batch 的时候,你是用 L1 来算你的 Gradient,你选到第二个 Batch 的时候,你</p>
<p>是用 L2 来算你的 Gradient,假设你用 L1 算 Gradient 的时候,发现 Gradient 是0,卡住了,但 L2 它的 Function 跟 L1 又不一</p>
<p>样,L2 就不一定会卡住,所以 L1 卡住了 没关系,换下一个 Batch 来,L2 再算 Gradient。</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315112841221.png" alt="image-20220315112841221"></p>
<p>小的batch_size不仅仅对训练有好处，在对测试的时候也有好处，下面也是一个实验，作者用了六个网络架构</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315114156470.png" alt="image-20220315114156470"></p>
<p>SB是小批次，LB是大批次，那么在他们训练的时候准确率都差不多的时候，进行测试，可以看到结果。</p>
<p>一个可能的解释</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315114913427.png" alt="image-20220315114913427"></p>
<p>上图是参数和loss二维图，实线是train，虚线是test，train和test的分布存在一定的偏差</p>
<p>那么一个说法是，<strong>大的 Batch Size,会让我们倾向走到峡谷里面,而小的 Batch Size,倾向让我们走到盆地里面</strong></p>
<p>那么在盆地里面的点容错性比较好。那么造成这样的原因的想法是这样的，和训练的时候差不多，小的batch有很多的loss，那么每次更新的方向都是不一样的，如果今天这个峡谷很窄，不小心走进去了，因为每次 Update 的方向都不太一样,它的 Update 的方向也就随机性,所以一个很小的峡谷,没有办法困住小的 Batch。</p>
<p>但这只是一个解释,那也不是每个人都相信这个解释,那这个其实还是一个<strong>尚待研究的问题</strong></p>
<p>省流助手：<strong>小的batch_size还是好的，就是速度可能慢一点</strong></p>
<p>总结</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315115518885.png" alt="image-20220315115518885"></p>
<h1 id="Momentum（关于更新方向）"><a href="#Momentum（关于更新方向）" class="headerlink" title="Momentum（关于更新方向）"></a>Momentum（关于更新方向）</h1><p>就是让咱们更新的方向带有“冲量”，这样在遇到鞍点或者局部极小点的时候，可以冲出去</p>
<p>简单讲，就是现在更新的方向，我们不仅仅看的是当前梯度的方向，还看上一步的方向。</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315152002018.png" alt="image-20220315152002018"></p>
<p>当加上 Momentum 的时候,我们 Update 的方向,不是只考虑现在的 Gradient,而是考虑过去所有 Gradient 的总合.</p>
<p>那么这样的话，在遇到鞍点或者局部极小点的时候，也可能会走出去</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315152501983.png" alt="image-20220315152501983"></p>
<p>如果有 Momentum 的话,你还是有办法继续走下去,因為 Momentum 不是只看 Gradient,Gradient 就算是 0,你还有前一步的方向,前一步的方向告诉我们向右走,我们就继续向右走,甚至你走到这种地方,Gradient 告诉你应该要往左走了,但是假设你前一步的影响力,比 Gradient 要大的话,你还是有可能继续往右走,甚至翻过一个小丘,搞不好就可以走到更好 Local Minima,这个就是 Momentum 有可能带来的好处。</p>
<h1 id="关于学习率"><a href="#关于学习率" class="headerlink" title="关于学习率"></a>关于学习率</h1><p>当我们的loss不再下降的时候，梯度这个时候真的会很小吗，当你今天训练一个network,train到后来发现,loss不再下降的时候,你不要随便说,我卡在local minima,我卡在saddle point,<strong>有时候根本两个都不是,你只是单纯的loss没有办法再下降</strong>也有可能是在两个两个谷壁间,不断的来回的震荡</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315153026724.png" alt="image-20220315153026724"></p>
<p>那么我们再来看一个具体的例子，就一个非常简单的模型，对于梯度下降来说是什么样的</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315153611969.png" alt="image-20220315153611969"></p>
<p>现在我们右下角的小黑点是起始点，然后黄色的X点是目标点，图中蓝色代表的是此时的w和b有小的loss，红色代表大的loss，那么我们在梯度下降的时候，学习率设的比较大，那么就有可能会出现这种情况，参数在山壁的两端不断的震荡。</p>
<p>那么我们想当然的把学习率调小一点</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315154829196.png" alt="image-20220315154829196"></p>
<p>出现的结果如上，终于从这个地方滑滑滑,滑到山谷底终于左转,但是你发现说,这个训练永远走不到终点,因為我的<strong>learning rate已经太小了</strong>,竖直往上这一段这个很斜的地方,因为这个坡度很陡,gradient的值很大,所以还能够前进一点,左拐以后这个地方坡度已经非常的平滑了,这么小的learning rate,根本没有办法再让我们的训练前进</p>
<p><strong>Different parameters needs different learning rate</strong></p>
<p>下面有几种方法：</p>
<h2 id="Root-Mean-Square"><a href="#Root-Mean-Square" class="headerlink" title="Root Mean Square"></a>Root Mean Square</h2><p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315155337996.png" alt="image-20220315155337996"></p>
<p>σᵢ 所有计算出来的gradient,它的平方和的平均再开根号</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>adagrad方法使用的就是root mean square</p>
<p>那么为啥会可以有梯度大的话，步长就小，梯度小，步长就大？</p>
<p>在梯度小的地方，然后这个σ是gradient的平方和取平均再开根号<br>$$<br>\sigma_i^t &#x3D; \sqrt{\frac{1}{t+1}\sum_{i&#x3D;0}^t(g_i^t)^2}<br>$$<br>所以算出来的σ就小,σ小 learning rate就大。反过来则同理，不过这个并不是最终的版本</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>adagrad的一个缺点就是，他在变化的时候不是很快，就是在需要调整学习率的时候，因为他是求和的，所以他会先走一会，然后再调整。</p>
<p>更进一步的版本：</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315161303356.png" alt="image-20220315161303356"></p>
<p>那这个<strong>α就像learning rate一样,这个你要自己调它,它是一个超参数hyperparameter</strong></p>
<ul>
<li><p>如果我今天<strong>α设很小趋近于0</strong>,就代表我觉得<strong>gᵢ¹相较于之前所算出来的gradient而言,比较重要</strong></p>
</li>
<li><p>我<strong>α设很大趋近于1</strong>,那就代表我觉得<strong>现在算出来的gᵢ¹比较不重要,之前算出来的gradient比较重要</strong></p>
</li>
</ul>
<p>那么我们在RMSProp里面</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315162600386.png" alt="image-20220315162600386"></p>
<p>如果不是RMS Prop,原来的Adagrad的话它反应比较慢,但如果你用RMS Prop,然后呢你把α设小一点,你就是让新的,刚看到的gradient影响比较大的话,那你就可以很快的让σ的值变大,也可以很快的让你的步伐变小。</p>
<p>也就是说让学习率可以更好的学会加速和刹车</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>最常用的其实就是Adam</p>
<p>Adam就是RMS Prop加上Momentum，框架基本上都封装好了，有一些参数使用默认值就可以得到不错的结果</p>
<p>那么进一步的改进，我们的学习率一直都是固定的，这个η是一个固定的值,learning rate scheduling的意思就是说,我们<strong>不要把η当一个常数,我们把它跟时间有关</strong></p>
<h2 id="Learning-Rate-Decay"><a href="#Learning-Rate-Decay" class="headerlink" title="Learning Rate Decay"></a>Learning Rate Decay</h2><p><strong>随著时间的不断地进行,随著参数不断的update,我们这个η让它越来越小</strong></p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315164646761.png" alt="image-20220315164646761"></p>
<p>（这个图为啥会喷出去我还是有点疑问哈哈哈）</p>
<p>常用的Learning Rate Scheduling的方式,叫做Warm Up</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315164708912.png" alt="image-20220315164708912"></p>
<p>这Warm Up的方法是<strong>让learning rate,要先变大后变小</strong>,你会问说 变大要变到多大呢,变大速度要多快呢 ，小速度要多快呢,<strong>这个也是hyperparameter</strong>,你要自己用手调的,但是大方向的大策略就是,learning rate要先变大后变小,那这个方法听起来很神奇,就是一个黑科技这样</p>
<p>这边有一个可能的解释是说,你想想看当我们在用Adam RMS Prop,或Adagrad的时候,我们会需要计算σ,它是一个统计的结果,<strong>σ告诉我们,某一个方向它到底有多陡,或者是多平滑</strong>,那这个统计的结果,<strong>要看得够多笔数据以后,这个统计才精准,所以一开始我们的统计是不精准的</strong></p>
<p>一开始我们的σ是不精准的,所以我们一开始不要让我们的参数,走离初始的地方太远,先让它在初始的地方呢,做一些像是探索这样,所以<strong>一开始learning rate比较小,是让它探索 收集一些有关error surface的情报</strong>,先收集有关σ的统计数据,<strong>等σ统计得比较精准以后,在让learning rate呢慢慢地爬升</strong></p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><p>那么假设现在一种情况是，数据有两个维度，参数也有两个，那么当这两个维度的数据相差很大的时候，比如一个维度的数据范围是0-1，另一个维度的数据范围是0-1000，就可能会出现下面的这种情况像下图所示</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315221052288.png" alt="image-20220315221052288"></p>
<p>因为当某一维度数据相对很大的时候，我们对参数进行一个小的更新，但因为数据是很大的，所以对loss产生的影响就是很大的，那么接着就影响的是梯度。</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315224224756.png" alt="image-20220315224224756"></p>
<p>如果是<strong>固定的 learning rate</strong>,你可能很难得到好的结果,所以我们才说你需要adaptive 的 learning rate、 Adam 等等比较进阶的 optimization 的方法,才能够得到好的结果</p>
<p>那么处理方法就是让数据normalization</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315223121002.png" alt="image-20220315223121002"></p>
<p>就是让这一维度的数据先减去这一维度的均值，再除以标准差</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315223258073.png" alt="image-20220315223258073"></p>
<p>那么这样之后的结果就是，这一维度的数据就分布在均值为0，标准差为1的数据分布</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315224251221.png" alt="image-20220315224251221"></p>
<p>但是我们还会考虑到，这里面除了对最开始的x需要做标准化处理，通过第一个隐层之后的数据也要再次进行标准化处理</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315224542878.png" alt="image-20220315224542878"></p>
<p>要注意到上图是<strong>批处理</strong>的情况下，这样的话，均值和标准差，都是结合了z1,z2,z3,所以当z1改变的时候，也会通过均值和标准差影响到z1^,z2^ ,z3^，所以这样的话就相当于是一个巨大的网络了。</p>
<p>同时要注意到，这样的方法应该是适用于batch相对比较大的情况，因为，只有batch相对比较大的时候，这一batch下的均值和标准差才有可能代表整体</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315230018387.png" alt="image-20220315230018387"></p>
<p>我们通常回加上γ和β，因为标准化之后的数据，均值为0，标准差为1，但可能真是的数据不是这样的，需要对此做一些限制，让机器自己去学习这些参数</p>
<p>那么在test的时候如何做normalization，因为有些线上的项目，在真正的运算的时候，不可能等到一批数据都来到了才做normalization。</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315230246740.png" alt="image-20220315230246740"></p>
<p>这个是在test的时候的处理，我们使用的均值和标准差是在训练的时候取得他们的均值和标准差的平均。这里面的p也是一个超参数</p>
<p>结果：</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220315230523127.png" alt="image-20220315230523127"></p>
<h1 id="如何来做分类任务"><a href="#如何来做分类任务" class="headerlink" title="如何来做分类任务"></a>如何来做分类任务</h1><p>我们预测某一件东西是哪种类别的，我们通常将类别变成one-hot</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316210748012.png" alt="image-20220316210748012"></p>
<p>那么我们的network也需要是输出三个数字才行</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316211021497.png" alt="image-20220316211021497"></p>
<p>那么我们对产生的[y1, y2, y3]，和class1，class2，class3，更接近哪一个，那么他就是属于哪个类别的</p>
<p>那么产生y1,y2,y3之后，我们往往还需要在进行softmax处理，也就是：</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316211603232.png" alt="image-20220316211603232"></p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316211658093.png" alt="image-20220316211658093"></p>
<p>那么这样处理之后，y1^, y2^, y3^他们的和是1，同时他们的数值都是在0-1之间，可以理解他们其实就是某一个类别的概率啦，上面的话就是是class1的几率是0.88，class2几率0.12</p>
<p>还会让大的值跟小的值的差距更大，softmax的输入往往叫做logit</p>
<p>如果是大于两个类别的时候，我们在此使用softmax，如果是两个类别的分类的话，那么这里你把softmax变换成sigmoid也是一样的效果</p>
<p>那么对于分类的loss，计算loss ，loss函数是cross-entroy，这个更常用在分类任务上</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316212951325.png" alt="image-20220316212951325"></p>
<p>那，为什么不可以是Mean Square Error呢？</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316212911022.png" alt="image-20220316212911022"></p>
<p>因为这个时候，你会卡在这</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220316213701308.png" alt="image-20220316213701308"></p>
<p>如果使用的是cross-entropy，那么我们可以比较顺利的走到右下角的参数</p>
<p>但是如果是MSE，Mean square error在这种Loss很大的地方,它是非常平坦的,它的gradient是非常小趋近于0的,如果你初始的时候在这个地方,离你的目标非常远,那它gradient又很小,你就会没有办法用gradient descent,顺利的走到右下角的地方去,</p>
<h1 id="模型评估标准"><a href="#模型评估标准" class="headerlink" title="模型评估标准"></a>模型评估标准</h1><p>原文：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u013063099/article/details/80964865">https://blog.csdn.net/u013063099/article/details/80964865</a></p>
<p>准确率、精确率、召回率、F1值</p>
<p><strong>TP、TN、FP、FN概念</strong></p>
<p>首先有关TP、TN、FP、FN的概念。大体来看，TP与TN都是分对了情况，TP是正类，TN是负类。则推断出，FP是把错的分成了对的，而FN则是把对的分成了错的。（我的记忆方法：首先看第一个字母是T则代表分类正确，反之分类错误；然后看P，在T中则是正类，若在F中则实际为负类分成了正的。）</p>
<p>【举例】一个班里有男女生，我们来进行分类，把女生看成正类，男生看成是负类。我们可以用混淆矩阵来描述TP、TN、FP、FN。</p>
<p><img src="C:\Users\83549\AppData\Roaming\Typora\typora-user-images\image-20220322034538581.png" alt="image-20220322034538581"></p>
<p><strong>准确率（Accuracy）。</strong>顾名思义，就是所有的预测正确（正类负类）的占总的比重。<br>$$<br>Accuracy &#x3D; \frac{TP + TN}{TP +TN + FP + FN}<br>$$</p>
<p><strong>精确率（Precision）</strong>，查准率。即正确预测为正的占全部预测为正的比例。个人理解：真正正确的占所有预测为正的比例。<br>$$<br>Precision &#x3D; \frac{TP}{TP + FP}<br>$$<br><strong>召回率（Recall）</strong>，即正确预测为正的占全部实际为正的比例。个人理解：真正正确的占所有实际为正的比例。<br>$$<br>Recall &#x3D; \frac{TP}{TP + FN}<br>$$<br><strong>F1值（H-mean值）</strong>。F1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。<br>$$<br>F_1 &#x3D; \frac{2TP}{2TP + FP + FN}<br>$$</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/26/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%88%AB"><span class="nav-number">1.</span> <span class="nav-text">机器学习的类别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-number">2.</span> <span class="nav-text">机器学习的步骤</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E7%82%B9%E5%92%8C%E5%85%A8%E5%B1%80%E6%9C%80%E5%B0%8F%E7%82%B9"><span class="nav-number">3.</span> <span class="nav-text">局部极小点和全局最小点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%9C%89%E5%BC%B9%E6%80%A7"><span class="nav-number">4.</span> <span class="nav-text">让模型更有弹性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E6%B7%B1%E5%BA%A6%E7%9A%84%E6%84%8F%E4%B9%89"><span class="nav-number">5.</span> <span class="nav-text">网络结构的深度的意义</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C%E4%B8%8D%E5%A5%BD%E6%80%8E%E4%B9%88%E5%8A%9E"><span class="nav-number">6.</span> <span class="nav-text">训练结果不好怎么办</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%EF%BC%8C%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%8C%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">7.</span> <span class="nav-text">训练集，验证集，测试集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#small-batch-%E5%92%8C-large-batch"><span class="nav-number">8.</span> <span class="nav-text">small batch 和 large batch</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Momentum%EF%BC%88%E5%85%B3%E4%BA%8E%E6%9B%B4%E6%96%B0%E6%96%B9%E5%90%91%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">Momentum（关于更新方向）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-number">10.</span> <span class="nav-text">关于学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Root-Mean-Square"><span class="nav-number">10.1.</span> <span class="nav-text">Root Mean Square</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">10.2.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">10.3.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">10.4.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-Rate-Decay"><span class="nav-number">10.5.</span> <span class="nav-text">Learning Rate Decay</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">11.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%9D%A5%E5%81%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="nav-number">12.</span> <span class="nav-text">如何来做分类任务</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86"><span class="nav-number">13.</span> <span class="nav-text">模型评估标准</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">wpb</p>
  <div class="site-description" itemprop="description">除了英俊其他没什么好说的</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wpb</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
